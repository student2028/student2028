<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>spark s3 测试 | 花逍遥</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">spark s3 测试</h1><a id="logo" href="/.">花逍遥</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">spark s3 测试</h1><div class="post-meta">2022-04-23<span> | </span><span class="category"><a href="/categories/Spark/">Spark</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85jdk8"><span class="toc-number">1.</span> <span class="toc-text">下载安装jdk8</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85spark"><span class="toc-number">2.</span> <span class="toc-text">下载安装spark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E8%AF%BB%E5%86%99s3%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">3.</span> <span class="toc-text">测试读写s3的数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95delta-lake"><span class="toc-number">4.</span> <span class="toc-text">测试delta lake</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-java-project-demo"><span class="toc-number">5.</span> <span class="toc-text">spark java project demo</span></a></li></ol></div></div><div class="post-content"><p>spark的本地安装与测试<br>环境：mac m1<br>安装当前最新版的spark3.2.1并进行简单的测试,整合hive metastore。<br>hive metastore的安装请参考前面一文章即可。</p>
<h4 id="下载安装jdk8"><a href="#下载安装jdk8" class="headerlink" title="下载安装jdk8"></a>下载安装jdk8</h4><p><a target="_blank" rel="noopener" href="https://www.oracle.com/java/technologies/downloads/#java8-mac">https://www.oracle.com/java/technologies/downloads/#java8-mac</a><br>user: <a href="mailto:&#104;&#110;&#121;&#x61;&#x6f;&#x78;&#x68;&#x40;&#49;&#x32;&#x36;&#x2e;&#99;&#111;&#x6d;">&#104;&#110;&#121;&#x61;&#x6f;&#x78;&#x68;&#x40;&#49;&#x32;&#x36;&#x2e;&#99;&#111;&#x6d;</a><br>pwd: xxxxxx<br>下载后一步一步安装即可。<br>因为之前安装了jdk18,所以现在有两个jdk版本。需要在.zshrc中指定JAVA_HOME<br>或者在spark相关的sh文件中指定也可以。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;/Users/student2028/app/spark/bin:<span class="variable">$PATH</span>&quot;</span></span><br><span class="line">JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_331.jdk/Contents/Home</span><br><span class="line">PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>:.</span><br><span class="line">CLASSPATH=<span class="variable">$JAVA_HOME</span>/lib/tools.jar:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:.</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME</span><br><span class="line"><span class="built_in">export</span> PATH</span><br><span class="line"><span class="built_in">export</span> CLASSPATH</span><br></pre></td></tr></table></figure>
<h4 id="下载安装spark"><a href="#下载安装spark" class="headerlink" title="下载安装spark"></a>下载安装spark</h4><p>如果下载without hadoop版本因为缺少各种jar包，所以需要自己补充进来。<br>同样使用prebuilt hadoop的版本，一定要两者兼容，否则也会报相关错误。<br>当前最新版本spark3.2兼容 hadoop3.3.1。如果版本不兼容，常报错误有</p>
<blockquote>
<p>Exception in thread “main” java.lang.NoSuchFieldError: JAVA_9</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz</span><br><span class="line">tar xvzf spark-3.2.1-bin-hadoop3.2.tgz</span><br><span class="line"><span class="built_in">rm</span> spark-3.2.1-bin-hadoop3.2.tgz</span><br><span class="line"><span class="built_in">ln</span> -s ./spark-3.2.1-bin-hadoop3.2 spark</span><br><span class="line"><span class="comment">##copy aws related jars to lib 注意此处的hadoop 需要是3.3版本</span></span><br><span class="line"><span class="built_in">cd</span> ~/app/hadoop</span><br><span class="line">find . -iname <span class="string">&quot;*aws*.jar&quot;</span> |xargs -I _ <span class="built_in">cp</span> _ ~/app/spark/jars</span><br></pre></td></tr></table></figure>
<h4 id="测试读写s3的数据"><a href="#测试读写s3的数据" class="headerlink" title="测试读写s3的数据"></a>测试读写s3的数据</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--conf spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled=<span class="literal">true</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key=minio \</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key=minio123 \</span><br><span class="line">--conf spark.hadoop.fs.s3a.endpoint=localhost:9000 \</span><br><span class="line">--conf spark.hadoop.fs.s3a.path.style.access=<span class="literal">true</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.connection.ssl.enabled=<span class="literal">false</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \</span><br><span class="line">--conf spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled=<span class="literal">true</span> </span><br><span class="line"></span><br><span class="line">val <span class="built_in">df</span>=Seq((1,<span class="string">&quot;stu1&quot;</span>,<span class="string">&quot;M&quot;</span>,33),(2,<span class="string">&quot;stu2&quot;</span>,<span class="string">&quot;F&quot;</span>,30)).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;gender&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.write.format(<span class="string">&quot;json&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;s3a://test/student&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.<br>  spark3.2开始提供了更好用的提交协议 ，请参考：<br>  <a target="_blank" rel="noopener" href="https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/">https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/</a></p>
</blockquote>
<blockquote>
<p>java.lang.ClassNotFoundException: org.apache.spark.internal.io.cloud.PathOutputCommitProtocol</p>
</blockquote>
<p>解决方案：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. <span class="built_in">cd</span>  ~/app/spark/jars/</span><br><span class="line">   wget https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.2.1/spark-hadoop-cloud_2.12-3.2.1.jar</span><br><span class="line"></span><br><span class="line">2. spark-shell --packages org.apache.spark:spark-hadoop-cloud_2.12:3.2.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Class org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider not found<br>这个问题的原因是spark预置的hadoop版本和hadoop-aws配置的版本不一致导致的，先看一下spark项目依赖的hadoop版本，<br>修正hadoop-aws的版本即可，一个方法是在maven项目里面，先去掉hadoop-aws依赖，<br>先看一下External Libraries 中搜一下hadoop即可以发现之前依赖的hadoop版本。</p>
</blockquote>
<h4 id="测试delta-lake"><a href="#测试delta-lake" class="headerlink" title="测试delta lake"></a>测试delta lake</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/app/spark/jars</span><br><span class="line">wget  https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.2.0/delta-core_2.12-1.2.0.jar</span><br><span class="line">wget  https://repo1.maven.org/maven2/io/delta/delta-storage/1.2.0/delta-storage-1.2.0.jar</span><br><span class="line"></span><br><span class="line">spark-shell \</span><br><span class="line">--conf <span class="string">&quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot;</span> \</span><br><span class="line">--conf <span class="string">&quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot;</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled=<span class="literal">true</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key=minio \</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key=minio123 \</span><br><span class="line">--conf spark.hadoop.fs.s3a.endpoint=localhost:9000 \</span><br><span class="line">--conf spark.hadoop.fs.s3a.path.style.access=<span class="literal">true</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.connection.ssl.enabled=<span class="literal">false</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \</span><br><span class="line">--conf spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled=<span class="literal">true</span> </span><br><span class="line"></span><br><span class="line">spark.range(5, 10).write.format(<span class="string">&quot;delta&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;s3a://test/delta1&quot;</span>)</span><br><span class="line">spark.read.format(<span class="string">&quot;delta&quot;</span>).load(<span class="string">&quot;s3a://test/delta1&quot;</span>).show(20)</span><br><span class="line">配合hivemetastore 创建表</span><br><span class="line">spark-sql \</span><br><span class="line">--conf <span class="string">&quot;spark.sql.warehouse.dir=s3a://hive/warehouse&quot;</span> \</span><br><span class="line">--conf <span class="string">&quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot;</span> \</span><br><span class="line">--conf <span class="string">&quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot;</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key=minio \</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key=minio123 \</span><br><span class="line">--conf spark.hadoop.fs.s3a.endpoint=localhost:9000 \</span><br><span class="line">--conf spark.hadoop.fs.s3a.path.style.access=<span class="literal">true</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.connection.ssl.enabled=<span class="literal">false</span> \</span><br><span class="line">--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \</span><br><span class="line">--conf spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled=<span class="literal">true</span> \</span><br><span class="line">--conf <span class="string">&quot;spark.hadoop.hive.metastore.uris=thrift://localhost:9083&quot;</span> </span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> test;</span><br><span class="line">use test;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test2;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test2 (id <span class="type">int</span>, name string) <span class="keyword">using</span> delta;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test2 <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;1&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test2 <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;22&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test2;</span><br></pre></td></tr></table></figure>

<h4 id="spark-java-project-demo"><a href="#spark-java-project-demo" class="headerlink" title="spark java project demo"></a>spark java project demo</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestDelta</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>,<span class="string">&quot;s3a://hive/warehouse&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled&quot;</span>,<span class="string">&quot;true&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.sql.catalogImplementation&quot;</span>,<span class="string">&quot;hive&quot;</span>);<span class="comment">//enabled hive support</span></span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.sql.extensions&quot;</span>,<span class="string">&quot;io.delta.sql.DeltaSparkSessionExtension&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.sql.catalog.spark_catalog&quot;</span>,<span class="string">&quot;org.apache.spark.sql.delta.catalog.DeltaCatalog&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.hadoop.hive.metastore.uris&quot;</span>,<span class="string">&quot;thrift://localhost:9083&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;spark.hadoop.hive.metastore.warehouse.dir&quot;</span>,<span class="string">&quot;s3a://hive/warehouse&quot;</span>);</span><br><span class="line"></span><br><span class="line">        sparkConf.set(<span class="string">&quot;fs.s3a.access.key&quot;</span>, <span class="string">&quot;minio&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;fs.s3a.secret.key&quot;</span>, <span class="string">&quot;minio123&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;fs.s3a.endpoint&quot;</span>,<span class="string">&quot;localhost:9000&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;fs.s3a.path.style.access&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;fs.s3a.connection.ssl.enabled&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        sparkConf.set(<span class="string">&quot;fs.s3a.impl&quot;</span>, <span class="string">&quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">SparkSession</span> <span class="variable">spark</span> <span class="operator">=</span> SparkSession.builder().config(sparkConf).master(<span class="string">&quot;local&quot;</span>).getOrCreate();</span><br><span class="line"><span class="comment">//        spark.sparkContext().setLogLevel(&quot;warn&quot;);</span></span><br><span class="line"><span class="comment">//        spark.sql(&quot;create database test&quot;);</span></span><br><span class="line"><span class="comment">//        spark.sql(&quot;use test&quot;);</span></span><br><span class="line"><span class="comment">//        spark.sql(&quot;drop table if exists test2&quot;);</span></span><br><span class="line"><span class="comment">//        spark.sql(&quot;create table if not exists test2(id int,name string)&quot;);</span></span><br><span class="line"><span class="comment">//        spark.sql(&quot;insert into test2 values(1,&#x27;1&#x27;)&quot;);</span></span><br><span class="line">        spark.sql(<span class="string">&quot;use test&quot;</span>);</span><br><span class="line">        spark.sql(<span class="string">&quot;show tables&quot;</span>).show();</span><br><span class="line">        spark.sql(<span class="string">&quot;select * from test.test2&quot;</span>).show();</span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.examples<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>sparktest<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.delta<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>delta-core_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hadoop-cloud_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-aws<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></div><div class="tags"><a href="/tags/大数据"><i class="fa fa-tag">大数据</i></a><a href="/tags/Spark"><i class="fa fa-tag">Spark</i></a></div><div class="post-nav"><a class="pre" href="/2022/04/23/hive-metastore-on-minio/">hive metastore on minio</a><a class="next" href="/2022/04/20/hexo/">Hexo博客</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hive/">Hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JS/">JS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9F%BA%E7%A1%80/">基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/JAVA/" style="font-size: 15px;">JAVA</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 15px;">面试题</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 15px;">博客</a> <a href="/tags/JS/" style="font-size: 15px;">JS</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E8%AF%AD%E5%BD%95/" style="font-size: 15px;">语录</a> <a href="/tags/%E7%94%9F%E6%B4%BB/" style="font-size: 15px;">生活</a> <a href="/tags/%E6%97%85%E8%A1%8C/" style="font-size: 15px;">旅行</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/04/23/hive-metastore-on-minio/">hive metastore on minio</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/23/spark-minio/">spark s3 测试</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/hexo/">Hexo博客</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/22/%E6%84%9F%E5%86%92123/">感冒123</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/20/%E7%A8%BB%E7%9B%9B%E7%9A%84%E6%B4%BB%E6%B3%95/">稻盛的活法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/14/%E5%A4%8F%E8%8A%B1%E5%AF%B9%E8%AF%9D%E5%BD%951/">夏花对话录1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/Scala%E5%AD%A6%E4%B9%A0Part3/">Scala学习Part3</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/Spark%E9%9D%A2%E8%AF%95%E9%A2%98Part1/">Spark面试题Part1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/Scala%E5%AD%A6%E4%B9%A0Part1/">Scala学习Part1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/Scala%E5%AD%A6%E4%B9%A02/">Scala学习Part2</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://www.cnblogs.com/huaxiaoyao/" title="博客园" target="_blank">博客园</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">花逍遥.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>