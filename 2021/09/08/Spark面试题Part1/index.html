<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>Spark面试题Part1 | 花逍遥</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark面试题Part1</h1><a id="logo" href="/.">花逍遥</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Spark面试题Part1</h1><div class="post-meta">2021-09-08<span> | </span><span class="category"><a href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD"><span class="toc-number">1.</span> <span class="toc-text">RDD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark%E4%BD%9C%E4%B8%9A%E7%9A%84%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">Spark作业的提交流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Shuffle%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">Shuffle过程分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-OOM%E5%88%86%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">Spark OOM分析</span></a></li></ol></div></div><div class="post-content"><p>spark core部分的重点<br>RDD的理解<br>shuffle部分的理解<br>spark作业的提交流程(DagScheduler,TaskScheduler,stage划分等)</p>
<h4 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">* <span class="type">A</span> <span class="type">Resilient</span> <span class="type">Distributed</span> <span class="type">Dataset</span> (<span class="type">RDD</span>), the basic abstraction in <span class="type">Spark</span>. <span class="type">Represents</span> an immutable,</span><br><span class="line">* partitioned collection of elements that can be operated on in parallel. <span class="type">This</span> <span class="class"><span class="keyword">class</span> <span class="title">contains</span> <span class="title">the</span></span></span><br><span class="line">* basic operations available on all <span class="type">RDDs</span>, such as `map`, `filter`, and `persist`. <span class="type">In</span> addition,</span><br><span class="line">* [[org.apache.spark.rdd.<span class="type">PairRDDFunctions</span>]] contains operations available only on <span class="type">RDDs</span> of key-value</span><br><span class="line">* pairs, such as `groupByKey` and `join`;</span><br><span class="line">* [[org.apache.spark.rdd.<span class="type">DoubleRDDFunctions</span>]] contains operations available only on <span class="type">RDDs</span> of</span><br><span class="line">* <span class="type">Doubles</span>; and</span><br><span class="line">* [[org.apache.spark.rdd.<span class="type">SequenceFileRDDFunctions</span>]] contains operations available on <span class="type">RDDs</span> that</span><br><span class="line">* can be saved as <span class="type">SequenceFiles</span>.</span><br><span class="line">* <span class="type">All</span> operations are automatically available on any <span class="type">RDD</span> of the right <span class="class"><span class="keyword">type</span> (<span class="params">e.g. <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span></span>)])</span></span><br><span class="line">* through <span class="keyword">implicit</span>.</span><br><span class="line"></span><br><span class="line">* <span class="type">Internally</span>, each <span class="type">RDD</span> is characterized by five main properties:</span><br><span class="line">*</span><br><span class="line">*  - <span class="type">A</span> list of partitions</span><br><span class="line">*  - <span class="type">A</span> function <span class="keyword">for</span> computing each split</span><br><span class="line">*  - <span class="type">A</span> list of dependencies on other <span class="type">RDDs</span></span><br><span class="line">*  - <span class="type">Optionally</span>, a <span class="type">Partitioner</span> <span class="keyword">for</span> key-value <span class="type">RDDs</span> (e.g. to say that the <span class="type">RDD</span> is hash-partitioned)</span><br><span class="line">*  - <span class="type">Optionally</span>, a list of preferred locations to compute each split on (e.g. block locations <span class="keyword">for</span></span><br><span class="line">*    an <span class="type">HDFS</span> file)</span><br></pre></td></tr></table></figure>
<h4 id="Spark作业的提交流程"><a href="#Spark作业的提交流程" class="headerlink" title="Spark作业的提交流程"></a>Spark作业的提交流程</h4><p>spark-submit<br>-&gt; 构据提交程序的主类,通过反射构造这个类实例<br>-&gt; 主类中的main静态方法开始运行<br>在这里面你构造的 SparkContext对象<br>它需要用到dagScheduler, taskScheudler, SparkEnv.<br>main代码里面 构造一个初始rdd,或多个rdd,<br>然后再做一些转换，最后触发一个或多个action.<br>由action开始构建一个job,分成stages,然后去运行。<br>sparkContext.submitjob -&gt; dagScheduler.submitJob<br>DAGSchedulerEventProcessLoop<br>eventProcessLoop.post(JobSubmitted(<br>jobId, rdd, func2, partitions.toArray, callSite, waiter,<br>Utils.cloneProperties(properties)))<br>dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions,<br>callSite, listener, properties)<br>finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)<br>val parents = getOrCreateParentStages(shuffleDeps, jobId)<br>val stage = new ResultStage(id, rdd, func, partitions, parents, jobId,<br>val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)<br>/** Submits stage, but first recursively submits any missing parents. */<br>val missing = getMissingParentStages(stage).sortBy(_.id)<br>for (parent &lt;- missing) {<br>submitStage(parent)<br>}<br>如果父RDD的一个Partition被一个子RDD的Partition所使用就是窄依赖，否则的话就是宽依赖。</p>
<p>// For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).<br>// For ResultTask, serialize and broadcast (rdd, func).<br>taskBinary = sc.broadcast(taskBinaryBytes)<br>根据stage的属性和partitions来构造tasks[]<br>taskScheduler.submitTasks(new TaskSet(<br>tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,<br>stage.resourceProfileId))<br>taskScheduler,taskset,maxTaskFailures构建成TasksetManager<br>设置任务集调度策略，调度模式有FAIR,FIFO两种，默认的是FIFO,将tasksetManager添加到<br>FIFOSchedulableBuilder中。</p>
<p>任务构造完成之后，分配到哪些节点？哪些节点有足够的资源。<br>资源分配，使用LocalBackend的reviveOffer方法，它的处理步骤如下：<br>1.使用ExecutorId,ExecutorHostName,freeCores创建WorkerOffer<br>2.调用TaskSchedulerImpl的resourceOffers方法分配资源<br>3.调用Executor的launchTask方法运行任务<br>Executor的launchTask方法，标志着任务执行阶段的开始。<br>1.创建TaskRunner,并将其与taskId,taskName以及serializedTask添加到runningTasks中。<br>2.TaskRunner实现了Runnable接口，最`1    后使用线程池来执行TaskRunner。<br>run方法的处理动作包含状态更新，任务反序列化和任务运行。<br>将driver提交的task在Executor上通过反序列化，更新依赖达到还原效果。<br>val(taskFiles, taskJars, taskBytes) = Task.deserializeWithDependencies(sd)<br>val value=task.run(taskId.toInt)</p>
<h4 id="Shuffle过程分析"><a href="#Shuffle过程分析" class="headerlink" title="Shuffle过程分析"></a>Shuffle过程分析</h4><p>shuffle过程用于连接map任务的输出和reduce任务的输入，map任务的中间输出结果按照key值<br>哈希后分配给某一个reduce任务。<br>shuffle包含map任务端的shuffle write和reduce任务端的shuffle read两部分。<br>writer部分的任务个数由finalRdd的partition个数决定。<br>reader部分的任务个数由spark.sql.shuffle.partitions个数决定。<br>write阶段会把状态与shuffle输出的数据数量位置信息封装到MapStatus对象，然后发送到driver.<br>reader阶段会从driver请求mapstatus,然后读取数据。</p>
<p>spark1.2之前使用hashsuffleManager来处理shuffle过程，现更新为sortShuffleManager来处理。<br>因为早期的HashshuffleManager在shuffle write阶段生成的中间结果文件过多，会给IO造成过多的压力。<br>1.map任务会为每一个reduce task创建一个bucket,map阶段最终会创建M*R个bucket。<br>2.reduce任务从本地或者远端的map任务所在的BlockManager获取相应的bucket作为输入。<br>早期shuffle过程存在的问题：<br>1.map任务的中间结果先存入内存，后写入磁盘，对内存的开销很大，当一个节点上的map输出很大时，<br>容易造成OOM.<br>2.生成的中间文件过多，磁盘IO将成为性能瓶颈。<br>spark shuffle做的优化如下：<br>1.将map任务给每个partition的reduce任务输出的bucket合并到同一个文件当中。<br>2.map任务逐条输出计算结果，使用appendOnlyMap缓存与聚合算法对中间结果进行聚合，减少中间结果<br>占用的内存大小。<br>3.对SizeTrackingAppendOnlyMap和SizeTrackingPairBuffer等缓存进行溢出判断，当超出<br>threshold时将数据写入磁盘，防止内存溢出。<br>4.reduce任务对拉取到的map任务中间结果逐条读是不是一次性读取到内存，并在内存中进行聚合和排序，<br>减少了对内存的使用。<br>shuffleSortManager 有两种模式，一种是bypass模式，一种是普通模式。<br>bypass模式启动的动件是shuffle read task个数小于200（bypassMergeThreshold)，同时mapsideCombine<br>是false的情况下，才会开启bypass,bypass模式基本可以理解为和之前shuffle模式一样，只是最后会合并生成的临时文件，<br>生成一个数据文件和一个索引文件。<br>普通模式的sortShuffleSortManager的工作流程：<br>在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。<br>如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；<br>如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着每写一条数据进入内存数据结构之后，<br>就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，<br>然后清空内存数据结构。</p>
<p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。<br>默认的batch数量是10000条，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。<br>写入磁盘文件是通过Java的BufferedOutputStream实现的。首先会将数据缓冲在内存中，<br>当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p>
<p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。<br>最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，<br>然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，<br>也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，<br>其中标识了下游各个task的数据在文件中的start offset与end offset。</p>
<p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">* <span class="type">Sort</span>-based shuffle has two different write paths <span class="keyword">for</span> producing its map output files:</span><br><span class="line">*  - <span class="type">Serialized</span> sorting: used when all three of the following conditions hold:</span><br><span class="line">*    <span class="number">1.</span> <span class="type">The</span> shuffle dependency specifies no map-side combine.</span><br><span class="line">*    <span class="number">2.</span> <span class="type">The</span> shuffle serializer supports relocation of serialized values (<span class="keyword">this</span> is currently</span><br><span class="line">*       supported by <span class="type">KryoSerializer</span> and <span class="type">Spark</span> <span class="type">SQL</span>&#x27;s custom serializers).</span><br><span class="line">*    <span class="number">3.</span> <span class="type">The</span> shuffle produces fewer than or equal to <span class="number">16777216</span> output partitions.</span><br><span class="line">*  - <span class="type">Deserialized</span> sorting: used to handle all other cases.</span><br><span class="line"><span class="type">ExternalSorter</span></span><br><span class="line"> *</span><br><span class="line">* <span class="meta">@param</span> aggregator optional <span class="type">Aggregator</span> <span class="keyword">with</span> combine functions to use <span class="keyword">for</span> merging data</span><br><span class="line">* <span class="meta">@param</span> partitioner optional <span class="type">Partitioner</span>; <span class="keyword">if</span> <span class="keyword">given</span>, sort by partition <span class="type">ID</span> and <span class="keyword">then</span> key</span><br><span class="line">* <span class="meta">@param</span> ordering optional <span class="type">Ordering</span> to sort keys within each partition; should be a total ordering</span><br><span class="line">* <span class="meta">@param</span> serializer serializer to use when spilling to disk</span><br></pre></td></tr></table></figure>

<h4 id="Spark-OOM分析"><a href="#Spark-OOM分析" class="headerlink" title="Spark OOM分析"></a>Spark OOM分析</h4><p>RDD已经可以序列化到磁盘，为什么我们Spark作业还会OOM?<br>合理地调整spark有关mergesort spill相关的参数</p>
</div><div class="tags"><a href="/tags/面试题"><i class="fa fa-tag">面试题</i></a><a href="/tags/Spark"><i class="fa fa-tag">Spark</i></a></div><div class="post-nav"><a class="pre" href="/2021/09/09/Scala%E5%AD%A6%E4%B9%A0Part3/">Scala学习Part3</a><a class="next" href="/2021/09/07/Scala%E5%AD%A6%E4%B9%A0Part1/">Scala学习Part1</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/JS/">JS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9F%BA%E7%A1%80/">基础</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/JAVA/" style="font-size: 15px;">JAVA</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 15px;">面试题</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Hbase/" style="font-size: 15px;">Hbase</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 15px;">大数据</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/JS/" style="font-size: 15px;">JS</a> <a href="/tags/%E8%AF%AD%E5%BD%95/" style="font-size: 15px;">语录</a> <a href="/tags/%E7%94%9F%E6%B4%BB/" style="font-size: 15px;">生活</a> <a href="/tags/%E6%97%85%E8%A1%8C/" style="font-size: 15px;">旅行</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/22/%E6%84%9F%E5%86%92123/">感冒123</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/20/%E7%A8%BB%E7%9B%9B%E7%9A%84%E6%B4%BB%E6%B3%95/">稻盛的活法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/14/%E5%A4%8F%E8%8A%B1%E5%AF%B9%E8%AF%9D%E5%BD%951/">夏花对话录1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/Scala%E5%AD%A6%E4%B9%A0Part3/">Scala学习Part3</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/Spark%E9%9D%A2%E8%AF%95%E9%A2%98Part1/">Spark面试题Part1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/Scala%E5%AD%A6%E4%B9%A0Part1/">Scala学习Part1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/Scala%E5%AD%A6%E4%B9%A02/">Scala学习Part2</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/04/%E7%A7%8B%E6%9E%AB/">秋枫</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/26/Part1/">大数据之Hbase Part1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/25/%E7%90%86%E8%A7%A3Java%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86Part1/">理解Java中的动态代理Part1</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://www.cnblogs.com/huaxiaoyao/" title="博客园" target="_blank">博客园</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">花逍遥.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>